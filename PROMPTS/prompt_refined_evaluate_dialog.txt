You must evaluate a generated dialog against:
- A reference human dialog
- A task specification

Rate each criterion from 1 (very bad) to 5 (excellent). Use the full range. Justify each rating briefly and clearly.

---

EXAMPLE DIALOG (STRICT EVALUATION)

Specification:
{"topic": "booking a holiday", "turns": 6, "participants": 2}

Generated dialog:
P1: Hi.
P2: Hello. Vacation.
P1: So. Sure.
P2: Yes.


Reference dialog:
P1: Hey, I was thinking we should finally book that trip to Lisbon.
P2: Yeah! Flights or Airbnb first?
P1: Let's check flights — maybe Skyscanner?
P2: Good idea. I'll search tonight and send options.
P1: Great, and I’ll take care of the hotel.
P2: Deal. We’re going to Portugal!

Evaluation:
-fluency: 2 -> The language lacks smoothness and syntactic completeness.
-coherence: 1 -> No logical progression; responses are disconnected and unmotivated.
-realism: 1 -> Unnatural and unconvincing; human conversations are never this vague.
-fidelity_to_specification: 2 -> Formal requirements met, but the topic is missing completely. Missed on the number of turns.
-engagement: 1 -> Flat and emotionless; there's nothing engaging in this exchange.
-originality: 1 -> Bland and content-free; lacks even the most basic creative input.
-comments: "An empty, robotic dialog that fails to express any purpose or relevance to the task."


---

Now evaluate the next dialog using the same structure and strictness.

Generated dialog:
{GENERATED_DIALOG_HERE}

Reference dialog:
{REFERENCE_DIALOG_HERE}

Specification:
{SPECIFICATION_HERE}

⚠️ Return ONLY a Python dictionary.
Example format:
{
  "fluency": 2,
  "coherence": 1,
  "realism": 1,
  "fidelity_to_specification": 2,
  "engagement": 1,
  "originality": 1,
  "comments": "An empty, robotic dialog that fails to express any purpose or relevance to the task."
